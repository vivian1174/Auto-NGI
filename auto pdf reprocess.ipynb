{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8558ab-4757-49c1-ae10-c7ce73401086",
   "metadata": {},
   "outputs": [],
   "source": [
    "################下方為將content_data依照title將上下文聯絡################先不要做"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11d130df-7abf-4780-a353-743e5ad42319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "處理後的內容已保存至 processed_content_data.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 讀取 content_data.txt 檔案\n",
    "with open('content_data.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# 儲存結果的列表\n",
    "processed_content = []\n",
    "current_title = \"\"\n",
    "current_paragraphs = []\n",
    "page_ref_pattern = re.compile(r'pg\\. (\\d+)')  # 用於檢測頁面引用的正則表達式\n",
    "\n",
    "# 處理每一行\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "\n",
    "    # 如果是標題行\n",
    "    if line.startswith(\"Title:\"):\n",
    "        # 如果之前有收集到段落，先保存\n",
    "        if current_title and current_paragraphs:\n",
    "            processed_content.append((current_title, \" \".join(current_paragraphs)))\n",
    "        \n",
    "        # 更新標題和清空段落\n",
    "        current_title = line.replace(\"Title: \", \"\")\n",
    "        current_paragraphs = []\n",
    "\n",
    "    # 如果是段落行，檢查是否包含頁面引用\n",
    "    elif line:\n",
    "        match = page_ref_pattern.search(line)\n",
    "        if match:\n",
    "            # 如果檢測到頁面引用，嘗試找出標題類似 \"MARKETS REPORT - Natural Gas Futures\" 的引用\n",
    "            if \"MARKETS REPORT\" in line:\n",
    "                current_paragraphs.append(line)  # 加入引用標記所在段落\n",
    "            # 連接前後段落\n",
    "            if current_paragraphs:\n",
    "                processed_content.append((current_title, \" \".join(current_paragraphs)))\n",
    "            current_paragraphs = []  # 重置段落收集\n",
    "        else:\n",
    "            current_paragraphs.append(line)\n",
    "\n",
    "# 如果最後還有段落未保存，保存它們\n",
    "if current_title and current_paragraphs:\n",
    "    processed_content.append((current_title, \" \".join(current_paragraphs)))\n",
    "\n",
    "# 將處理後的結果寫入檔案\n",
    "with open('processed_content_data.txt', 'w', encoding='utf-8') as output_file:\n",
    "    for title, paragraphs in processed_content:\n",
    "        output_file.write(f\"Title: {title}\\n\")\n",
    "        output_file.write(f\"{paragraphs}\\n\\n\")\n",
    "\n",
    "print(\"處理後的內容已保存至 processed_content_data.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a85ad6-ac89-4ec5-a079-37710c55fc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################顯示價格及存檔牛逼###############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae47b110-af03-462c-8314-4fff817289cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-07T01:13:34.216844Z",
     "iopub.status.busy": "2025-02-07T01:13:34.216844Z",
     "iopub.status.idle": "2025-02-07T01:13:34.466713Z",
     "shell.execute_reply": "2025-02-07T01:13:34.466713Z",
     "shell.execute_reply.started": "2025-02-07T01:13:34.216844Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Henry Hub Price: 3.310\n",
      "Columbia Gulf Mainline Price: 3.160\n",
      "Texas Gas Zone 1 Price: 3.145\n",
      "標題和段落數據已保存至 C:\\Users\\N000189549\\Desktop\\Python\\auto price import\\auto price import\\reprocessed txt and json\\NGI daily index_20250207_content_data.txt，表格數據已保存至 C:\\Users\\N000189549\\Desktop\\Python\\auto price import\\auto price import\\reprocessed txt and json\\NGI daily index_20250207_tables_data.json\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os\n",
    "\n",
    "# 讀取 HTML 檔案\n",
    "html_file = r'C:\\Users\\N000189549\\Desktop\\Python\\auto price import\\auto price import\\reprocessed txt and json\\NGI daily index_20250207.html'\n",
    "with open(html_file, 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# 解析 HTML\n",
    "soup = BeautifulSoup(html_content, 'lxml')\n",
    "\n",
    "# 儲存結果的列表\n",
    "content_data = []\n",
    "tables_data = []\n",
    "\n",
    "# 1. 提取標題和段落\n",
    "for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p']):\n",
    "    if tag.name.startswith('h'):  # 判斷是否為標題\n",
    "        # 當前標題\n",
    "        title = tag.get_text(strip=True)\n",
    "        # 獲取後續的段落\n",
    "        next_elements = tag.find_next_siblings()  # 獲取所有後續元素\n",
    "        paragraphs = []\n",
    "\n",
    "        for elem in next_elements:\n",
    "            if elem.name == 'p':  # 當前元素是 <p>\n",
    "                paragraphs.append(elem.get_text(strip=True))\n",
    "            elif elem.name in ['table', 'img']:  # 排除 <table> 或 <img>\n",
    "                continue\n",
    "            else:  # 當前元素不是 <p>，則停止搜尋\n",
    "                break\n",
    "\n",
    "        # 將標題和其後的段落放入列表中\n",
    "        content_data.append((title, paragraphs))\n",
    "\n",
    "# 如果沒有找到標題，就將所有文本按順序存儲\n",
    "if not content_data:\n",
    "    paragraphs = [p.get_text(strip=True) for p in soup.find_all('p') if not (p.find('table') or p.find('img'))]\n",
    "    content_data = [('No Title', paragraphs)]\n",
    "\n",
    "# 2. 提取表格數據\n",
    "tables = soup.find_all('table')\n",
    "\n",
    "if tables:\n",
    "    for i, table in enumerate(tables):\n",
    "        table_rows = []\n",
    "        rows = table.find_all('tr')\n",
    "        \n",
    "        for row in rows:\n",
    "            # 提取每一行的每一個單元格\n",
    "            cols = row.find_all(['td', 'th'])  # 包含 <td> 和 <th> 單元格\n",
    "            cols = [col.get_text(strip=True) for col in cols]  # 獲取文本並去除多餘空白\n",
    "            table_rows.append(cols)  # 將行數據添加到表格列表\n",
    "        \n",
    "        tables_data.append({f\"Table {i+1}\": table_rows})  # 將表格數據存入結果列表\n",
    "\n",
    "# 提取價格數據\n",
    "henry_hub_price = None\n",
    "columbia_gulf_price = None\n",
    "texas_gas_zone_price = None\n",
    "\n",
    "henry_hub_index = None\n",
    "columbia_gulf_index = None\n",
    "\n",
    "# 計算 Henry Hub 與 Columbia Gulf Mainline 的距離\n",
    "min_distance = float('inf')\n",
    "\n",
    "# 尋找價格\n",
    "for table in tables_data:\n",
    "    for key, rows in table.items():\n",
    "        for index, row in enumerate(rows):\n",
    "            if isinstance(row, list) and len(row) >= 2:\n",
    "                # 查找 Columbia Gulf Mainline\n",
    "                if \"Columbia Gulf Mainline\" in row:\n",
    "                    columbia_gulf_price = row[row.index(\"Columbia Gulf Mainline\") + 2]\n",
    "                    columbia_gulf_index = index  # 記錄 Columbia Gulf Mainline 的行號\n",
    "                \n",
    "                # 查找 Henry Hub 價格\n",
    "                if \"Henry Hub\" in row:\n",
    "                    henry_hub_temp_price = row[row.index(\"Henry Hub\") + 2]  # 暫時獲取 Henry Hub 價格\n",
    "                    henry_hub_temp_index = index  # 暫時記錄 Henry Hub 的行號\n",
    "                    \n",
    "                    # 如果已經找到 Columbia Gulf Mainline，則計算與之的距離\n",
    "                    if columbia_gulf_index is not None:\n",
    "                        distance = abs(henry_hub_temp_index - columbia_gulf_index)\n",
    "                        # 如果這個 Henry Hub 比較近，則更新最終的價格和行號\n",
    "                        if distance < min_distance:\n",
    "                            min_distance = distance\n",
    "                            henry_hub_price = henry_hub_temp_price\n",
    "                            henry_hub_index = henry_hub_temp_index\n",
    "\n",
    "                # 查找 Texas Gas Zone 1\n",
    "                if \"Texas Gas Zone 1\" in row:\n",
    "                    texas_gas_zone_price = row[row.index(\"Texas Gas Zone 1\") + 2]\n",
    "\n",
    "# 輸出提取的價格\n",
    "print(f\"Henry Hub Price: {henry_hub_price}\")\n",
    "print(f\"Columbia Gulf Mainline Price: {columbia_gulf_price}\")\n",
    "print(f\"Texas Gas Zone 1 Price: {texas_gas_zone_price}\")\n",
    "\n",
    "# 生成輸出檔案的名稱\n",
    "base_filename = os.path.splitext(os.path.basename(html_file))[0]\n",
    "output_dir = r\"C:\\Users\\N000189549\\Desktop\\Python\\auto price import\\auto price import\\reprocessed txt and json\"\n",
    "\n",
    "content_data_file = os.path.join(output_dir, f\"{base_filename}_content_data.txt\")\n",
    "tables_data_file = os.path.join(output_dir, f\"{base_filename}_tables_data.json\")\n",
    "\n",
    "# 3. 將標題和段落數據寫入 TXT 檔案\n",
    "with open(content_data_file, 'w', encoding='utf-8') as txt_file:\n",
    "    for title, paragraphs in content_data:\n",
    "        txt_file.write(f\"Title: {title}\\n\")  # 寫入標題\n",
    "        for paragraph in paragraphs:\n",
    "            txt_file.write(f\"{paragraph}\\n\")  # 寫入段落\n",
    "        txt_file.write(\"\\n\")  # 在每個標題後加一行空白\n",
    "\n",
    "# 4. 將表格數據寫入 JSON 檔案\n",
    "with open(tables_data_file, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(tables_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# 結果通知\n",
    "print(f\"標題和段落數據已保存至 {content_data_file}，表格數據已保存至 {tables_data_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c711cc7b-6e5c-4b79-9ccd-5f41bb181784",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
